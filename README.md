# Data Lake â€” Medallion Architecture Project

A fully local Data Lake pipeline built on the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) orchestrated by **Apache Airflow**. Three independent fact tables are generated, cleaned, and aggregated through each layer.

---

## Architecture Overview

```
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚      Apache Airflow              â”‚
                          â”‚                                  â”‚
                          â”‚  DAG 1: generator_dag (*/5 min)  â”‚
                          â”‚    â”œâ”€â”€ generate_sales            â”‚
                          â”‚    â”œâ”€â”€ generate_customer_events  â”‚
                          â”‚    â””â”€â”€ generate_inventory        â”‚
                          â”‚                                  â”‚
                          â”‚  DAG 2: pipeline_dag (*/30 min)  â”‚
                          â”‚    â””â”€â”€ bronze_to_silver          â”‚
                          â”‚         â””â”€â”€ silver_to_gold       â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  local_output/           (staging: raw CSVs before bronze)             â”‚
â”‚    â”œâ”€â”€ sales/            â”‚  customer_events/  â”‚  inventory/            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ copy (save_to_bronze)
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  datalake/bronze/<domain>/year=YYYY/month=MM/day=DD/*.csv              â”‚
â”‚  Raw, unmodified files. Nothing is ever deleted here.                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ clean + validate (bronze_to_silver)
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  datalake/silver/<domain>/year=YYYY/month=MM/day=DD/*.parquet          â”‚
â”‚  Deduplicated, type-cast, validated. is_valid + validation_errors cols â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ aggregate (silver_to_gold)
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  datalake/gold/                                                        â”‚
â”‚    â”œâ”€â”€ daily_sales_summary/          â”œâ”€â”€ category_sales_summary/       â”‚
â”‚    â”œâ”€â”€ payment_method_summary/       â”œâ”€â”€ customer_activity_summary/    â”‚
â”‚    â”œâ”€â”€ device_usage_summary/         â”œâ”€â”€ inventory_movement_summary/   â”‚
â”‚    â””â”€â”€ inventory_net_position/                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Fact Tables

| Domain | Generator | Rows/batch | Key Fields |
|--------|-----------|-----------|------------|
| ğŸ›’ Sales | `sales_generator.py` | 10 | sale_id, customer_id, product, category, quantity, unit_price, total_amount, status |
| ğŸ‘¤ Customer Events | `customer_events_generator.py` | 15 | event_id, session_id, event_type, customer_id, device_type, page_url |
| ğŸ“¦ Inventory | `inventory_generator.py` | 8 | movement_id, warehouse_id, product_id, movement_type, quantity, unit_cost |

---

## Project Structure

```
data-lake-medallion-architecture-project/
â”‚
â”œâ”€â”€ config.py                        # Paths, domain names, intervals
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ generator/
â”‚   â”œâ”€â”€ sales_generator.py
â”‚   â”œâ”€â”€ customer_events_generator.py
â”‚   â””â”€â”€ inventory_generator.py
â”‚
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ local_storage.py             # Bronze/Silver/Gold read-write helpers
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ bronze_to_silver.py          # Clean + validate
â”‚   â””â”€â”€ silver_to_gold.py           # Aggregate KPIs
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ data_lake_pipeline.py       # Airflow DAG definitions (2 DAGs)
â”‚
â”œâ”€â”€ local_output/                    # Auto-created; gitignored
â”œâ”€â”€ datalake/                        # Auto-created; gitignored
â””â”€â”€ .state/                          # Auto-created; gitignored
```

---

## Setup

### 1. Choose your environment

This project uses two separate virtual environments to handle different use cases:

*   **`.venv` (Windows Native)**: For running generators and pipelines manually in PowerShell. (Already created)
*   **`.venv_linux` (WSL/Ubuntu)**: For running Airflow 3.0+ orchestration. (Already created)

To activate them:
- **Windows**: `.venv\Scripts\Activate.ps1`
- **WSL**: `source .venv_linux/bin/activate`

### 2. Install dependencies (if adding new ones)
```bash
pip install -r requirements.txt
```

### 3. Set the Airflow home and PYTHONPATH

```powershell
$env:AIRFLOW_HOME = "$PWD\airflow_home"
$env:PYTHONPATH   = "$PWD"
```

### 3. Initialise Airflow (v3.0+)

In Airflow 3.0+, user management is handled via the configuration. The `admin` user is already defined in your `airflow.cfg`.

```bash
# Initialize the DB
airflow db migrate
```

> [!NOTE]
> The `airflow users create` command was removed in v3.0. For the default Simple Auth Manager, users are defined in `airflow.cfg` under `[core] simple_auth_manager_users`. On the first run, Airflow will generate a password file in your `AIRFLOW_HOME`.


### 4. Copy DAGs folder into Airflow home

```powershell
# Tell Airflow where to find our DAGs
$env:AIRFLOW__CORE__DAGS_FOLDER = "$PWD\dags"
```

Or add to `airflow_home/airflow.cfg`:
```ini
[core]
dags_folder = <absolute path to project>\dags
```

---

## Running

### Option A â€” Airflow (full orchestration)

> [!IMPORTANT]
> **Windows Compatibility**: Airflow requires a POSIX environment and cannot be run natively on Windows. To use the Airflow webserver and scheduler, please use **WSL2** (Windows Subsystem for Linux), or use **Option B** below for native Windows execution.

Open **two PowerShell windows** (within WSL2):

```powershell
# Window 1 â€” API / Web server
airflow api-server --port 8080

# Window 2 â€” Scheduler
airflow scheduler
```

Open `http://localhost:8080`, log in with `admin / admin`, and enable:
- `data_lake_generator_dag`  â†’ runs every 5 minutes
- `data_lake_pipeline_dag`   â†’ runs every 30 minutes

### Option B â€” Run scripts manually (no Airflow)

```powershell
# Set PYTHONPATH so imports work
$env:PYTHONPATH = "$PWD"

# Step 1 â€” Generate data + push to Bronze (all 3 generators)
python -m generator.sales_generator
python -m generator.customer_events_generator
python -m generator.inventory_generator

# Step 2 â€” Bronze â†’ Silver
python -m pipeline.bronze_to_silver

# Step 3 â€” Silver â†’ Gold
python -m pipeline.silver_to_gold
```

---

## Verifying Output

```powershell
# Check staging CSVs
Get-ChildItem local_output -Recurse -Filter *.csv

# Check Bronze layer
Get-ChildItem datalake\bronze -Recurse -Filter *.csv

# Check Silver layer (Parquet)
Get-ChildItem datalake\silver -Recurse -Filter *.parquet

# Check Gold layer (Parquet)
Get-ChildItem datalake\gold -Recurse -Filter *.parquet
```

**Inspect a Gold table in Python:**

```python
import pandas as pd
import glob

files = sorted(glob.glob("datalake/gold/daily_sales_summary/*.parquet"))
df = pd.read_parquet(files[-1])   # latest snapshot
print(df.to_string())
```

---

## Silver Validation Rules

| Domain | Check |
|--------|-------|
| sales | Null check on sale_id, customer_id, product_id, quantity, unit_price, total_amount |
| sales | `total_amount == quantity Ã— unit_price` (auto-corrected if both operands present) |
| customer_events | Null check on event_id, customer_id, session_id, event_type |
| customer_events | `event_type` must be one of: login, browse, add_to_cart, checkout, logout |
| inventory | Null check on movement_id, product_id, warehouse_id, movement_type, quantity |
| inventory | `movement_type` must be one of: inbound, outbound, adjustment |
| inventory | `quantity` must be a positive number |

Invalid rows are **kept** in Silver with `is_valid = False` and a `validation_errors` description â€” they flow to Silver but are excluded from Gold aggregations.

---

## Gold Tables

| Table | Source | Description |
|-------|--------|-------------|
| `daily_sales_summary` | silver/sales | Revenue, order count, avg order value per day |
| `category_sales_summary` | silver/sales | Revenue breakdown by product category per day |
| `payment_method_summary` | silver/sales | Revenue by payment method per day |
| `customer_activity_summary` | silver/customer_events | Event counts by type per day |
| `device_usage_summary` | silver/customer_events | Session counts by device type per day |
| `inventory_movement_summary` | silver/inventory | Qty moved per product/warehouse/type per day |
| `inventory_net_position` | silver/inventory | Net stock (inbound âˆ’ outbound) per product/warehouse |